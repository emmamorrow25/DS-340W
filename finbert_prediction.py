# -*- coding: utf-8 -*-
"""finbert_prediction_update.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kppwNh4k5WbwcXFAJgKHEj6xcjcvm1Ac
"""

!pip install emoji

import emoji

import pandas as pd
import numpy as np
import re
import torch

df = pd.read_csv('./drive/MyDrive/stocktwits_dataset/SPY_concat_files.csv')


def process_text(r):
  texts = r["Sentence"]
  # remove URLs
  texts = re.sub(r'https?://\S+', "", texts)
  texts = re.sub(r'www.\S+', "", texts)
  # remove '
  texts = texts.replace('&#39;', "'")
  texts = texts.replace('\n', " ")
  # remove symbol names
  texts = re.sub(r'(\#)(\S+)', r'hashtag_\2', texts)
  texts = re.sub(r'(\$)([A-Za-z]+)', r'cashtag_\2', texts)
  # remove usernames
  texts = re.sub(r'(\@)(\S+)', r'mention_\2', texts)
  # demojize
  texts = emoji.demojize(texts, delimiters=("", " "))

  return texts.strip()

df["Sentence"] = df.apply(process_text, axis=1)

df.shape

df = df[ df['Sentence'].apply(lambda x: len(x.split(' '))>2) ]

df = df[ df['Sentence'].apply(lambda x: len(x)<512) ]

df.shape

!pip install transformers

#Getting the tokenizer and the model

from transformers import AutoTokenizer, AutoModelForSequenceClassification
  
tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")

model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")

def predict_sentiment(text):
  inputs = tokenizer([text], padding = True, truncation = True, return_tensors='pt')
  outputs = model(**inputs)
  predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
  result = np.argmax( predictions.detach().numpy(), axis=1 )
  z = [model.config.id2label[zi] for zi in result]
  return z[0]

sentiments = []
for index, row in df.iterrows():

  if ((index + 1) % 10 == 0):
      print(f"{index+1}/{len(df)}, {int(index/len(df)*100)}%")
  s = row["Sentence"]
  sentiments.append(predict_sentiment(s))

df["Sentiment"] = sentiments

df.to_csv('FinBERT.csv')

print("Finished.")